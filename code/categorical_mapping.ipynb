{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2ec381-d810-45bb-9a75-97235a146ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6161f291-a4cf-4cd0-b0e3-f67ffd3c82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml  # or json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, shared_memory\n",
    "import os\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477e1c0-1eb4-489d-933f-88fb8031d971",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Synthetic Population Generation via Categorical Constraint Matching**\n",
    "\n",
    "#### **Core Methodology**\n",
    "This implementation transforms raw census constraints and individual microdata into optimized numerical representations for synthetic population generation, using a three-stage process:\n",
    "\n",
    "1. **Constraint Processing**  \n",
    "   - Ingests structured CSV constraints (e.g., `age.csv`, `sex.csv`)  \n",
    "   - Generates:  \n",
    "     - A **label list** (e.g., `['age%16-24', 'age%25-34', 'sex%m', 'sex%f']`)  \n",
    "     - A **target matrix** of census counts per geography zone (`np.array` shape: `[n_zones, n_categories]`)  \n",
    "\n",
    "2. **Microdata Encoding**  \n",
    "   - Converts individual records (`microdata.csv`) into a sparse binary matrix where:  \n",
    "     - Rows = Individuals  \n",
    "     - Columns = Constraint categories  \n",
    "     - Values = `1` (matches category) or `0` (no match/missing)  \n",
    "   - *Example*: An individual with `age=25-34` and `sex=m` encodes as `[0,1,0,0,0,0,1,0]`  \n",
    "\n",
    "3. **Memory-Efficient Design**  \n",
    "   - Uses pure NumPy arrays (no Pandas) for:  \n",
    "     - Zero-copy sharing in multiprocessing  \n",
    "     - O(1) incremental updates during annealing  \n",
    "   - Handles missing data implicitly via zero-padding  \n",
    "\n",
    "#### **Key Innovations**  \n",
    "- **Deterministic Labeling**: Human-readable category prefixes (`age%`, `sex%`) ensure traceability  \n",
    "- **Sparse-by-Design**: Binary encoding minimizes memory overhead  \n",
    "- **Annealing-Ready**: Optimized for rapid constraint violation checks during optimization  \n",
    "\n",
    "#### **Technical Highlights**  \n",
    "```python\n",
    "# Pseudocode of data flow\n",
    "constraint_labels, constraint_targets = build_constraint_arrays(config)  # From age/sex CSVs\n",
    "microdata_encoded = encode_microdata(config, constraint_labels)  # Binary matrix\n",
    "\n",
    "# During annealing:\n",
    "current_error = calculate_error(microdata_encoded, constraint_targets)  # L1/Chi-squared\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of Data Flow**  \n",
    "```mermaid\n",
    "graph LR\n",
    "    A[age.csv] --> C[Constraint Processor]\n",
    "    B[sex.csv] --> C\n",
    "    C --> D[Constraint Labels]\n",
    "    C --> E[Target Matrix]\n",
    "    F[microdata.csv] --> G[Microdata Encoder]\n",
    "    D --> G\n",
    "    G --> H[Binary Encoded Matrix]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Works**  \n",
    "- **Scalability**: Processes 100K+ individuals with minimal memory  \n",
    "- **Flexibility**: New constraints require only YAML updates (no code changes)  \n",
    "- **Reproducibility**: Explicit category mapping avoids hidden assumptions  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3762880-a62b-4129-b3b5-371c23665830",
   "metadata": {},
   "source": [
    "## Example YAML Configuration\n",
    "\n",
    "```yaml\n",
    "# Required microdata source\n",
    "microdata:\n",
    "  file: \"data/microdata.csv\"  # Path to individual records\n",
    "  id_column: \"ID\"            # Optional unique identifier column\n",
    "\n",
    "# List of constraint definitions\n",
    "constraints:\n",
    "  # Age distribution constraints\n",
    "  - file: \"data/age.csv\"             # Census data file\n",
    "    microdata_id: \"Age\"              # Matching column in microdata\n",
    "    constraint_prefix: \"Age%\"        # Label prefix for categories\n",
    "    geography_column: \"GEO_CODE\"    # Zone identifier column\n",
    "\n",
    "  # Sex distribution constraints  \n",
    "  - file: \"data/sex.csv\"\n",
    "    microdata_id: \"Sex\"\n",
    "    constraint_prefix: \"Sex%\"\n",
    "    geography_column: \"GEO_CODE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56aea048-460e-4b81-b922-e0da212706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load YAML config file and validate structure.\"\"\"\n",
    "    config_path = Path(config_path)\n",
    "    with open(config_path) as f:\n",
    "        if config_path.suffix == '.yaml':\n",
    "            config = yaml.safe_load(f)\n",
    "        else:\n",
    "            import json\n",
    "            config = json.load(f)\n",
    "    \n",
    "    # Validate config structure\n",
    "    assert \"microdata\" in config, \"Config missing 'microdata' section\"\n",
    "    assert \"constraints\" in config and len(config[\"constraints\"]) > 0, \"No constraints defined\"\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8a919-346a-432c-9c0b-dc165ac9a24d",
   "metadata": {},
   "source": [
    "## `load_config(config_path)`\n",
    "\n",
    "**Purpose**:  \n",
    "Loads and validates a configuration file (YAML or JSON) that defines the microdata and constraints structure.\n",
    "\n",
    "**Inputs**:\n",
    "- `config_path` (str or Path): Path to the configuration file (`.yaml` or `.json`)\n",
    "\n",
    "**Returns**:\n",
    "- `dict`: Parsed configuration with keys `'microdata'` and `'constraints'`\n",
    "\n",
    "**Key Features**:\n",
    "- Automatically detects file format (YAML/JSON) from extension\n",
    "- Validates presence of required sections:\n",
    "  - `'microdata'`: File path for microdata CSV\n",
    "  - `'constraints'`: List of constraint definitions\n",
    "- Raises `AssertionError` if structure is invalid\n",
    "\n",
    "**Example Config**:\n",
    "```yaml\n",
    "microdata:\n",
    "  file: \"data/microdata.csv\"\n",
    "constraints:\n",
    "  - file: \"data/age.csv\"\n",
    "    microdata_id: \"age\"\n",
    "    constraint_prefix: \"age%\"\n",
    "    geography_column: \"GEO_CODE\"\n",
    "    set_as_population_total: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3006ffb3-d570-49e8-87c0-4112a957ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'microdata': {'file': 'testdata/microdata.csv', 'id_column': 'ID'}, 'constraints': [{'file': 'testdata/age.csv', 'microdata_id': 'Age', 'constraint_prefix': 'Age%', 'geography_column': 'GEO_CODE', 'set_as_population_total': True}, {'file': 'testdata/sex.csv', 'microdata_id': 'Sex', 'constraint_prefix': 'Sex%', 'geography_column': 'GEO_CODE', 'set_as_population_total': False}]}\n"
     ]
    }
   ],
   "source": [
    "config = load_config('testdata/config.yaml')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a4b2232-1bf9-4c48-abd6-7afe43d746ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_constraint_arrays(config):\n",
    "    \"\"\"\n",
    "    Enhanced version that:\n",
    "    1. Uses set_as_population_total to calculate population sizes\n",
    "    2. Tracks geography codes (GEOIDs) separately\n",
    "    3. Returns results in a structured dict\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"constraint_labels\": List[str],\n",
    "            \"constraint_targets\": np.array,\n",
    "            \"geography_codes\": List[str],\n",
    "            \"population_totals\": np.array\n",
    "        }\n",
    "    \"\"\"\n",
    "    constraint_labels = []\n",
    "    constraint_targets = None\n",
    "    geography_codes = []\n",
    "    pop_total_constraint = False\n",
    "    population_totals = []\n",
    "\n",
    "    for constraint in config[\"constraints\"]:\n",
    "        with open(constraint[\"file\"], mode='r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = next(reader)\n",
    "            data = list(reader)\n",
    "        \n",
    "        poptotal_constraint = constraint[\"set_as_population_total\"]\n",
    "        print(poptotal_constraint)\n",
    "            \n",
    "        geo_col = constraint[\"geography_column\"]\n",
    "        geo_idx = headers.index(geo_col)\n",
    "        \n",
    "        # Store GEOIDs on first pass\n",
    "        if not geography_codes:\n",
    "            geography_codes = [row[geo_idx] for row in data]\n",
    "        \n",
    "        # Handle population totals if specified\n",
    "        if pop_total_constraint: \n",
    "            population_totals = np.array([float(row[pop_idx]) for row in data])\n",
    "        \n",
    "        # Process categories\n",
    "        categories = [h for i, h in enumerate(headers) if i != geo_idx]\n",
    "        prefix = constraint[\"constraint_prefix\"]\n",
    "        constraint_labels.extend(f\"{prefix}{cat}\" for cat in categories)\n",
    "        \n",
    "        # Extract targets\n",
    "        target_rows = []\n",
    "        for row in data:\n",
    "            target_values = [float(row[i]) for i in range(len(headers)) if i != geo_idx]\n",
    "            target_rows.append(target_values)\n",
    "            if poptotal_constraint:\n",
    "                population_totals.append(sum(target_values))\n",
    "        \n",
    "        targets = np.array(target_rows)\n",
    "        constraint_targets = targets if constraint_targets is None else np.hstack([constraint_targets, targets])\n",
    "    \n",
    "    return {\n",
    "        \"constraint_labels\": constraint_labels,\n",
    "        \"constraint_targets\": constraint_targets,\n",
    "        \"geography_codes\": geography_codes,\n",
    "        \"population_totals\": population_totals \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99e5001d-8552-4b9b-8a73-0b8749b351b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "constraints_dict = build_constraint_arrays(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca277644-7a5d-4850-b043-2fd19e37eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'constraint_labels': ['Age%16-24',\n",
       "  'Age%25-34',\n",
       "  'Age%35-44',\n",
       "  'Age%45-54',\n",
       "  'Age%55-64',\n",
       "  'Age%65-74',\n",
       "  'Sex%m',\n",
       "  'Sex%f'],\n",
       " 'constraint_targets': array([[ 1350.,  1531.,  2086.,  2443.,  2304.,  1631.,  5572.,  5773.],\n",
       "        [ 1719.,  2423.,  2472.,  2713.,  2457.,  1638.,  6683.,  6739.],\n",
       "        [ 1694.,  1826.,  2480.,  2875.,  2446.,  1811.,  6372.,  6760.],\n",
       "        [ 1997.,  2287.,  2218.,  2250.,  1645.,  1069.,  5652.,  5814.],\n",
       "        [ 2789.,  3535.,  2824.,  1982.,  1533.,   947.,  6868.,  6742.],\n",
       "        [ 3124.,  3740.,  2839.,  1795.,  1314.,   704.,  6769.,  6747.],\n",
       "        [ 6715.,  5114.,  2725.,  1534.,  1154.,   668., 10125.,  7785.],\n",
       "        [ 1931.,  2194.,  2364.,  2176.,  1852.,  1058.,  5434.,  6141.],\n",
       "        [ 1392.,  1677.,  2211.,  2480.,  2335.,  1849.,  5764.,  6180.],\n",
       "        [ 2124.,  2607.,  2283.,  2449.,  1899.,  1178.,  5976.,  6564.],\n",
       "        [ 2196.,  2845.,  2369.,  1936.,  1628.,   918.,  5839.,  6053.],\n",
       "        [ 2198.,  2679.,  2334.,  1730.,  1442.,   871.,  5584.,  5670.],\n",
       "        [ 1606.,  2128.,  2498.,  2564.,  2027.,  1310.,  6006.,  6127.],\n",
       "        [ 1125.,  1076.,  1930.,  2223.,  1940.,  1625.,  4778.,  5141.],\n",
       "        [ 2372.,  3023.,  2240.,  1694.,  1669.,   991.,  6125.,  5864.],\n",
       "        [ 1727.,  2193.,  2370.,  2297.,  2012.,  1416.,  5967.,  6048.],\n",
       "        [ 1941.,  1994.,  2266.,  2343.,  1856.,  1377.,  5682.,  6095.],\n",
       "        [ 2855.,  4005.,  2830.,  1906.,  1374.,   856.,  6978.,  6848.],\n",
       "        [ 2947.,  3839.,  2710.,  1585.,  1326.,   772.,  6929.,  6250.],\n",
       "        [ 1812.,  2179.,  2524.,  2573.,  1852.,  1097.,  5917.,  6120.],\n",
       "        [ 2003.,  2416.,  2403.,  2344.,  1797.,  1171.,  5765.,  6369.],\n",
       "        [ 1501.,  2537.,  2350.,  2084.,  1836.,  1157.,  5808.,  5657.],\n",
       "        [ 1838.,  2325.,  2456.,  2365.,  1995.,  1267.,  5922.,  6324.],\n",
       "        [ 2918.,  3761.,  2644.,  1668.,  1360.,   788.,  6652.,  6487.],\n",
       "        [ 2440.,  3189.,  2741.,  2700.,  1941.,  1192.,  6782.,  7421.],\n",
       "        [  881.,   939.,  1752.,  1885.,  1673.,  1175.,  4017.,  4288.],\n",
       "        [ 1616.,  1876.,  2070.,  2059.,  1641.,  1121.,  5031.,  5352.],\n",
       "        [ 1846.,  2233.,  2353.,  2279.,  1855.,  1256.,  5786.,  6036.],\n",
       "        [ 1295.,  1571.,  2233.,  2400.,  2131.,  1215.,  5362.,  5483.],\n",
       "        [ 1681.,  1889.,  2043.,  2167.,  1978.,  1360.,  5452.,  5666.],\n",
       "        [ 1109.,  1234.,  1584.,  1729.,  1525.,  1111.,  4161.,  4131.],\n",
       "        [  989.,   999.,  1931.,  2163.,  1859.,   965.,  4276.,  4630.],\n",
       "        [ 1246.,  1479.,  1683.,  1689.,  1429.,  1050.,  4200.,  4376.],\n",
       "        [ 1029.,  1246.,  1840.,  1878.,  1555.,   948.,  4231.,  4265.],\n",
       "        [ 1007.,  1201.,  1586.,  1781.,  1629.,  1091.,  4123.,  4172.],\n",
       "        [ 1425.,  1431.,  1846.,  1838.,  1570.,  1158.,  4522.,  4746.],\n",
       "        [ 1030.,   924.,  1569.,  1814.,  1533.,  1013.,  3881.,  4002.],\n",
       "        [ 1085.,  1066.,  1678.,  1911.,  1630.,  1229.,  4230.,  4369.],\n",
       "        [ 1570.,  1663.,  1727.,  1765.,  1331.,   869.,  4317.,  4608.],\n",
       "        [ 2233.,  2714.,  2082.,  1356.,  1163.,   667.,  5192.,  5023.],\n",
       "        [ 1123.,  1128.,  1674.,  1605.,  1579.,  1250.,  4190.,  4169.],\n",
       "        [ 1076.,  1156.,  1783.,  1857.,  1545.,  1005.,  4173.,  4249.],\n",
       "        [ 1235.,  1664.,  1852.,  1793.,  1512.,  1069.,  4566.,  4559.],\n",
       "        [ 1289.,  1609.,  1626.,  1741.,  1491.,  1070.,  4421.,  4405.],\n",
       "        [ 1225.,  1405.,  1822.,  1808.,  1674.,  1008.,  4403.,  4539.],\n",
       "        [ 1394.,  1868.,  1675.,  1768.,  1358.,   996.,  4724.,  4335.],\n",
       "        [ 1255.,  1677.,  1605.,  1585.,  1421.,  1094.,  4212.,  4425.],\n",
       "        [ 2315.,  2693.,  2678.,  2579.,  2094.,  1556.,  6711.,  7204.],\n",
       "        [ 2412.,  3189.,  2663.,  2162.,  1821.,  1188.,  6878.,  6557.],\n",
       "        [ 2316.,  2893.,  2628.,  2241.,  2085.,  1465.,  6759.,  6869.],\n",
       "        [ 1654.,  1940.,  2358.,  2394.,  2128.,  1493.,  5827.,  6140.],\n",
       "        [ 1723.,  1999.,  2485.,  2400.,  2205.,  1565.,  6199.,  6178.],\n",
       "        [ 2246.,  2182.,  2429.,  2538.,  2032.,  1507.,  6385.,  6549.],\n",
       "        [ 1408.,  1531.,  2462.,  2416.,  2456.,  1816.,  6002.,  6087.],\n",
       "        [ 2315.,  2514.,  2726.,  2593.,  2086.,  1500.,  6934.,  6800.],\n",
       "        [ 2563.,  2556.,  2496.,  2302.,  1912.,  1335.,  6679.,  6485.],\n",
       "        [ 2677.,  3342.,  2773.,  2041.,  1561.,   903.,  6740.,  6557.],\n",
       "        [ 2068.,  2590.,  2740.,  2649.,  2099.,  1323.,  6683.,  6786.],\n",
       "        [ 3040.,  3425.,  2819.,  2303.,  1768.,  1252.,  7696.,  6911.],\n",
       "        [ 1993.,  2386.,  2438.,  2214.,  2006.,  1270.,  6159.,  6148.],\n",
       "        [ 1632.,  1565.,  2889.,  3220.,  2820.,  1811.,  6947.,  6990.],\n",
       "        [ 1905.,  2214.,  2782.,  2861.,  2388.,  1785.,  6776.,  7159.],\n",
       "        [ 2018.,  2360.,  2891.,  2676.,  2474.,  1822.,  7002.,  7239.],\n",
       "        [ 1888.,  1924.,  2688.,  2846.,  2734.,  2251.,  7087.,  7244.],\n",
       "        [ 4706.,  3014.,  2361.,  2200.,  2009.,  1332.,  8226.,  7396.],\n",
       "        [ 1589.,  1703.,  2611.,  3015.,  2888.,  2151.,  6774.,  7183.],\n",
       "        [ 2163.,  2867.,  3178.,  3234.,  2848.,  1990.,  7849.,  8431.],\n",
       "        [ 2260.,  3042.,  3758.,  3390.,  2457.,  1627.,  8090.,  8444.],\n",
       "        [ 3334.,  5149.,  3720.,  3195.,  2258.,  1495.,  9982.,  9169.],\n",
       "        [ 2715.,  3878.,  3075.,  2828.,  2069.,  1432.,  8082.,  7915.],\n",
       "        [ 2384.,  3589.,  3124.,  2774.,  2269.,  1504.,  7552.,  8092.],\n",
       "        [ 3599.,  4341.,  3595.,  2914.,  2226.,  1403.,  9233.,  8845.],\n",
       "        [ 2086.,  3631.,  3440.,  3018.,  2669.,  1834.,  8123.,  8555.],\n",
       "        [ 3103.,  4919.,  3503.,  2910.,  1936.,  1282.,  8471.,  9182.],\n",
       "        [10425.,  9661.,  3489.,  2295.,  1699.,  1030., 15256., 13343.],\n",
       "        [ 2331.,  2779.,  2832.,  2943.,  2894.,  2147.,  7631.,  8295.],\n",
       "        [ 2675.,  3742.,  3473.,  3368.,  2619.,  1844.,  8656.,  9065.],\n",
       "        [ 1972.,  1815.,  2828.,  2836.,  2578.,  2430.,  7008.,  7451.]]),\n",
       " 'geography_codes': ['E05001341',\n",
       "  'E05001342',\n",
       "  'E05001343',\n",
       "  'E05001344',\n",
       "  'E05001345',\n",
       "  'E05001346',\n",
       "  'E05001347',\n",
       "  'E05001348',\n",
       "  'E05001349',\n",
       "  'E05001350',\n",
       "  'E05001351',\n",
       "  'E05001352',\n",
       "  'E05001353',\n",
       "  'E05001354',\n",
       "  'E05001355',\n",
       "  'E05001356',\n",
       "  'E05001357',\n",
       "  'E05001358',\n",
       "  'E05001359',\n",
       "  'E05001360',\n",
       "  'E05001361',\n",
       "  'E05001362',\n",
       "  'E05001363',\n",
       "  'E05001364',\n",
       "  'E05001365',\n",
       "  'E05001366',\n",
       "  'E05001367',\n",
       "  'E05001368',\n",
       "  'E05001369',\n",
       "  'E05001370',\n",
       "  'E05001371',\n",
       "  'E05001372',\n",
       "  'E05001373',\n",
       "  'E05001374',\n",
       "  'E05001375',\n",
       "  'E05001376',\n",
       "  'E05001377',\n",
       "  'E05001378',\n",
       "  'E05001379',\n",
       "  'E05001380',\n",
       "  'E05001381',\n",
       "  'E05001382',\n",
       "  'E05001383',\n",
       "  'E05001384',\n",
       "  'E05001385',\n",
       "  'E05001386',\n",
       "  'E05001387',\n",
       "  'E05001389',\n",
       "  'E05001390',\n",
       "  'E05001391',\n",
       "  'E05001392',\n",
       "  'E05001393',\n",
       "  'E05001396',\n",
       "  'E05001397',\n",
       "  'E05001398',\n",
       "  'E05001399',\n",
       "  'E05001400',\n",
       "  'E05001401',\n",
       "  'E05001402',\n",
       "  'E05001403',\n",
       "  'E05001405',\n",
       "  'E05001407',\n",
       "  'E05001408',\n",
       "  'E05001409',\n",
       "  'E05001410',\n",
       "  'E05001411',\n",
       "  'E05001412',\n",
       "  'E05001413',\n",
       "  'E05001414',\n",
       "  'E05001415',\n",
       "  'E05001416',\n",
       "  'E05001417',\n",
       "  'E05001418',\n",
       "  'E05001419',\n",
       "  'E05001420',\n",
       "  'E05001421',\n",
       "  'E05001422',\n",
       "  'E05001423'],\n",
       " 'population_totals': [11345.0,\n",
       "  13422.0,\n",
       "  13132.0,\n",
       "  11466.0,\n",
       "  13610.0,\n",
       "  13516.0,\n",
       "  17910.0,\n",
       "  11575.0,\n",
       "  11944.0,\n",
       "  12540.0,\n",
       "  11892.0,\n",
       "  11254.0,\n",
       "  12133.0,\n",
       "  9919.0,\n",
       "  11989.0,\n",
       "  12015.0,\n",
       "  11777.0,\n",
       "  13826.0,\n",
       "  13179.0,\n",
       "  12037.0,\n",
       "  12134.0,\n",
       "  11465.0,\n",
       "  12246.0,\n",
       "  13139.0,\n",
       "  14203.0,\n",
       "  8305.0,\n",
       "  10383.0,\n",
       "  11822.0,\n",
       "  10845.0,\n",
       "  11118.0,\n",
       "  8292.0,\n",
       "  8906.0,\n",
       "  8576.0,\n",
       "  8496.0,\n",
       "  8295.0,\n",
       "  9268.0,\n",
       "  7883.0,\n",
       "  8599.0,\n",
       "  8925.0,\n",
       "  10215.0,\n",
       "  8359.0,\n",
       "  8422.0,\n",
       "  9125.0,\n",
       "  8826.0,\n",
       "  8942.0,\n",
       "  9059.0,\n",
       "  8637.0,\n",
       "  13915.0,\n",
       "  13435.0,\n",
       "  13628.0,\n",
       "  11967.0,\n",
       "  12377.0,\n",
       "  12934.0,\n",
       "  12089.0,\n",
       "  13734.0,\n",
       "  13164.0,\n",
       "  13297.0,\n",
       "  13469.0,\n",
       "  14607.0,\n",
       "  12307.0,\n",
       "  13937.0,\n",
       "  13935.0,\n",
       "  14241.0,\n",
       "  14331.0,\n",
       "  15622.0,\n",
       "  13957.0,\n",
       "  16280.0,\n",
       "  16534.0,\n",
       "  19151.0,\n",
       "  15997.0,\n",
       "  15644.0,\n",
       "  18078.0,\n",
       "  16678.0,\n",
       "  17653.0,\n",
       "  28599.0,\n",
       "  15926.0,\n",
       "  17721.0,\n",
       "  14459.0]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd8a71-80c9-4f90-a775-5fbb9ac185b0",
   "metadata": {},
   "source": [
    "## `build_constraint_arrays(config)`\n",
    "\n",
    "**Purpose**:  \n",
    "Transforms constraint CSV files (like age/sex distributions) into labeled NumPy arrays for synthetic population generation.\n",
    "\n",
    "### Inputs\n",
    "- `config` (dict): Configuration dictionary containing:\n",
    "  - `constraints`: List of constraint definitions (file paths, prefixes, etc.)\n",
    "\n",
    "### Returns\n",
    "- `constraint_labels` (List[str]): Formatted category labels  \n",
    "  Example: `[\"age%16-24\", \"age%25-34\", \"sex%m\", \"sex%f\"]`\n",
    "- `constraint_targets` (np.array): 2D array of census counts  \n",
    "  Shape: `(n_geographies, n_constraints)`\n",
    "\n",
    "### Key Features\n",
    "-  **File Processing**:\n",
    "  - Reads CSV files without Pandas (vanilla Python `csv` module)\n",
    "  - Handles header rows and geography columns intelligently\n",
    "-  **Smart Labeling**:\n",
    "  - Combines constraint prefixes with category names  \n",
    "    (e.g., `\"age%\" + \"25-34\" â†’ \"age%25-34\"`)\n",
    "-  **Array Construction**:\n",
    "  - Builds a consolidated NumPy array by horizontally stacking constraints\n",
    "  - Automatically converts string values to floats\n",
    "\n",
    "### Example Workflow\n",
    "```python\n",
    "config = {\n",
    "    \"constraints\": [\n",
    "        {\n",
    "            \"file\": \"data/age.csv\",\n",
    "            \"constraint_prefix\": \"age%\",\n",
    "            \"geography_column\": \"GEO_CODE\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "labels, targets = build_constraint_arrays(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d7a37-fcda-4c48-917c-3b17d21e30a0",
   "metadata": {},
   "source": [
    "#### **Key Points**\n",
    "- **`np.hstack`**: Short for \"horizontal stack,\" it concatenates arrays column-wise.  \n",
    "  Example:\n",
    "  ```python\n",
    "  import numpy as np\n",
    "  a = np.array([[1, 2], [3, 4]])\n",
    "  b = np.array([[5], [6]])\n",
    "  np.hstack([a, b])  # Result: [[1, 2, 5], [3, 4, 6]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3cbf0f0-07c3-4616-b828-1bada3b1a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_microdata(config, constraint_labels):\n",
    "    \"\"\"\n",
    "    Encode microdata into a one-hot-like array where missing values are 0.\n",
    "    Returns:\n",
    "        microdata_encoded: np.array shape (n_individuals, n_constraints)\n",
    "        ids: list of IDs from the microdata\n",
    "    \"\"\"\n",
    "    # Step 1: Load microdata from CSV\n",
    "    with open(config[\"microdata\"][\"file\"], mode='r') as f:\n",
    "        reader = csv.DictReader(f)  # Reads header and rows as dictionaries\n",
    "        microdata = list(reader)    # Convert to list of dicts\n",
    "\n",
    "    n_individuals = len(microdata)\n",
    "    n_constraints = len(constraint_labels)\n",
    "\n",
    "    # Step 2: Create label-to-index mapping\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(constraint_labels)}\n",
    "\n",
    "    # Step 3: Initialize output array (all zeros)\n",
    "    microdata_encoded = np.zeros((n_individuals, n_constraints), dtype=np.int8)\n",
    "\n",
    "    # Step 4: Extract IDs\n",
    "    ids = [row[config[\"microdata\"][\"id_column\"]] for row in microdata]\n",
    "\n",
    "    # Step 5: Encode each constraint\n",
    "    for constraint in config[\"constraints\"]:\n",
    "        col = constraint[\"microdata_id\"]\n",
    "        prefix = constraint[\"constraint_prefix\"]\n",
    "\n",
    "        for row_idx, row in enumerate(microdata):\n",
    "            value = row.get(col)  # Get value for the current constraint column\n",
    "\n",
    "            # Skip missing values (leave as 0)\n",
    "            if value is not None and value.strip() != '':  # Check for non-empty strings\n",
    "                label = f\"{prefix}{value}\"\n",
    "                if label in label_to_idx:  # Ensure label exists in constraints\n",
    "                    microdata_encoded[row_idx, label_to_idx[label]] = 1\n",
    "\n",
    "    return {\n",
    "        \"microdata_encoded\":microdata_encoded, \n",
    "        \"ids\":ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7e3cf36-63cf-4eaf-9836-947d2eb80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "microdata_dict = encode_microdata(config,constraints_dict[\"constraint_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc89c4-42e4-48ef-93f3-41a7fabeb49c",
   "metadata": {},
   "source": [
    "##  `encode_microdata(config, constraint_labels)`\n",
    "\n",
    "**Purpose**:  \n",
    "Converts individual microdata records into a binary matrix matching census constraints, with automatic handling of missing values.\n",
    "\n",
    "### Inputs\n",
    "- `config` (dict): Configuration dictionary with microdata file path\n",
    "- `constraint_labels` (List[str]): Pre-generated labels from `build_constraint_arrays()`\n",
    "\n",
    "### Returns\n",
    "- `microdata_encoded` (np.array): Binary matrix where:\n",
    "  - Rows = Individuals\n",
    "  - Columns = Constraint categories\n",
    "  - Values = `1` (present) or `0` (missing/not applicable)\n",
    "\n",
    "### Key Features\n",
    "-   **Smart Encoding**:\n",
    "  - Converts categorical values (e.g., `\"m\"`, `\"25-34\"`) to binary flags\n",
    "  - Preserves relationships between original data and constraint categories\n",
    "-   **Missing Data Handling**:\n",
    "  - Empty/missing values remain `0` by default\n",
    "  - Silent skipping of undefined categories\n",
    "-   **Efficient Construction**:\n",
    "  - Pre-allocates NumPy array for performance\n",
    "  - Uses memory-efficient `int8` dtype\n",
    "\n",
    "### Example Transformation\n",
    "**Input Microdata**:\n",
    "```csv\n",
    "ID,sex,age\n",
    "1,m,25-34\n",
    "2,f,55-64\n",
    "3,,45-54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f89d307f-7abd-4225-8861-fdcbab056853",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('testdata/microdata_encoded.csv', microdata_dict[\"microdata_encoded\"], delimiter=',')\n",
    "np.savetxt('testdata/constraint_targets.csv', constraints_dict[\"constraint_targets\"], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "888650a4-2a0f-44f7-97c1-e3b4f06c8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelAnnealingGenerator:\n",
    "    def __init__(self, target_distribution, population_size, micro_data, \n",
    "                 initial_temp=100.0, cooling_rate=0.99):\n",
    "        \"\"\"\n",
    "        Parallel-safe implementation with proper annealing controls\n",
    "        \n",
    "        Args:\n",
    "            target_distribution: 1D np.array of target category counts\n",
    "            population_size: Number of individuals to select\n",
    "            micro_data: Shared read-only microdata array\n",
    "            initial_temp: Starting temperature (default 100.0)\n",
    "            cooling_rate: Geometric cooling rate (default 0.99)\n",
    "        \"\"\"\n",
    "        self.target = target_distribution\n",
    "        self.pop_size = int(population_size)\n",
    "        self.full_data = micro_data  # Shared read-only reference\n",
    "        \n",
    "        # Annealing controls\n",
    "        self.temp = float(initial_temp)\n",
    "        self.base_temp = float(initial_temp)\n",
    "        self.cooling = float(cooling_rate)\n",
    "        \n",
    "        # Initialize population (protected copy)\n",
    "        self.indices = np.random.choice(len(micro_data), size=self.pop_size, replace=True)\n",
    "        self.local_pop = micro_data[self.indices].copy()  # Isolated working copy\n",
    "        self.counts = self.local_pop.sum(axis=0)\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_indices = self.indices.copy()\n",
    "        self.best_counts = self.counts.copy()\n",
    "        self.best_error = self._calculate_error(self.counts)\n",
    "\n",
    "    def _calculate_error(self, counts):\n",
    "        \"\"\"Compute L1 distance between current and target distributions\"\"\"\n",
    "        return np.sum(np.abs(counts - self.target))\n",
    "\n",
    "    def optimization_step(self):\n",
    "        # 1. Select replacement candidate\n",
    "        replace_pos = np.random.randint(self.pop_size)\n",
    "        old_features = self.local_pop[replace_pos]\n",
    "        \n",
    "        # 2. Get new candidate safely from shared array\n",
    "        new_idx = np.random.randint(len(self.full_data))\n",
    "        new_features = self.full_data[new_idx]  # Atomic read\n",
    "        \n",
    "        # 3. Calculate delta\n",
    "        delta = new_features - old_features\n",
    "        new_counts = self.counts + delta\n",
    "        new_error = self._calculate_error(new_counts)\n",
    "        error_delta = new_error - self.best_error\n",
    "        \n",
    "        # 4. Metropolis acceptance\n",
    "        if error_delta < 0 or np.random.rand() < np.exp(-error_delta / self.temp):\n",
    "            self.counts = new_counts\n",
    "            self.local_pop[replace_pos] = new_features\n",
    "            self.indices[replace_pos] = new_idx\n",
    "            \n",
    "            if new_error < self.best_error:\n",
    "                self.best_error = new_error\n",
    "                self.best_indices = self.indices.copy()\n",
    "                self.best_counts = self.counts.copy()\n",
    "        \n",
    "        # 5. Apply cooling\n",
    "        self.temp *= self.cooling\n",
    "        return self.best_error\n",
    "\n",
    "    def reset_annealing(self, new_temp=None):\n",
    "        \"\"\"Reset temperature for new runs\"\"\"\n",
    "        self.temp = float(new_temp) if new_temp else self.base_temp\n",
    "        # Optional: Keep best solution or re-randomize\n",
    "        # self.indices = self.best_indices.copy()\n",
    "\n",
    "    def get_results(self):\n",
    "        return {\n",
    "            'selected_indices': self.best_indices,\n",
    "            'population_counts': self.best_counts,\n",
    "            'target_counts': self.target,\n",
    "            'error': self.best_error,\n",
    "            'final_temperature': self.temp\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae439d-858b-44ce-a5cb-f2823922df21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb1f231c-f73d-44a7-a8c3-acd0cf7f32ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage pattern matching your example:\n",
    "print(\"start\")\n",
    "for i in range(len(constraints_dict[\"geography_codes\"])):\n",
    "    # Initialize generator for this geography\n",
    "    generator = ParallelAnnealingGenerator(\n",
    "        constraints_dict[\"constraint_targets\"][i],\n",
    "        constraints_dict[\"population_totals\"][i],  # Note: Fixed typo from your original\n",
    "        microdata_dict[\"microdata_encoded\"],\n",
    "        100,  # Initial temp\n",
    "        0.99   # Cooling rate\n",
    "    )\n",
    "    \n",
    "    # Initial error threshold (10% of first step)\n",
    "    threshold = generator.optimization_step() / 10\n",
    "    \n",
    "    # Run until convergence or max iterations\n",
    "    for c in range(20000):\n",
    "        current_error = generator.optimization_step()\n",
    "        if current_error <= threshold :#or generator.temp <0.000001:\n",
    "            break\n",
    "    \n",
    "    results = generator.get_results()\n",
    "    # print(f\"Geography {constraints_dict['geography_codes'][i]}:\")\n",
    "    # print(f\"- Final error: {results['error']:.1f}\")\n",
    "    # print(f\"pop: {len(results['selected_indices'])} - Sample indices: {results['selected_indices'][:5]} \")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "594de7c9-1f18-47e4-bc2d-9541dda34645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  # For progress tracking\n",
    "\n",
    "# def init_worker(shared_mem_name, shape, dtype):\n",
    "#     \"\"\"Initialize worker with shared memory\"\"\"\n",
    "#     global shared_microdata\n",
    "#     shm = shared_memory.SharedMemory(name=shared_mem_name)\n",
    "#     shared_microdata = np.ndarray(shape, dtype=dtype, buffer=shm.buf)\n",
    "\n",
    "# def worker(args):\n",
    "#     geo_idx, target, pop_size = args\n",
    "#     try:\n",
    "#         # Initialize with more aggressive cooling\n",
    "#         generator = ParallelAnnealingGenerator(\n",
    "#             target_distribution=target,\n",
    "#             population_size=pop_size,\n",
    "#             micro_data=shared_microdata,\n",
    "#             initial_temp=50.0,  # Lower initial temp\n",
    "#             cooling_rate=0.95    # Faster cooling\n",
    "#         )\n",
    "        \n",
    "#         # Dynamic threshold based on population size\n",
    "#         threshold = max(5, pop_size * 0.01)  # At least 5, max 1% of pop size\n",
    "        \n",
    "#         # Optimized stopping conditions\n",
    "#         for step in range(20000):\n",
    "#             print(step,geo_idx)\n",
    "#             error = generator.optimization_step()\n",
    "#             if error <= threshold or generator.temp < 0.1:\n",
    "#                 break\n",
    "                \n",
    "#         return {\n",
    "#             'geo_idx': geo_idx,\n",
    "#             'error': error,\n",
    "#             'steps': step,\n",
    "#             'final_temp': generator.temp\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in geography {geo_idx}: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Prepare shared memory (10-20% faster than automatic pickling)\n",
    "#     microdata = microdata_dict[\"microdata_encoded\"]\n",
    "#     shm = shared_memory.SharedMemory(create=True, size=microdata.nbytes)\n",
    "#     shared_array = np.ndarray(microdata.shape, dtype=microdata.dtype, buffer=shm.buf)\n",
    "#     shared_array[:] = microdata[:]\n",
    "\n",
    "#     try:\n",
    "#         # Optimal worker count (leave 1 core free)\n",
    "#         n_workers = max(1, os.cpu_count() - 1)\n",
    "#         print(f\"Using {n_workers} workers with shared memory\")\n",
    "        \n",
    "#         with Pool(\n",
    "#             processes=n_workers,\n",
    "#             initializer=init_worker,\n",
    "#             initargs=(shm.name, microdata.shape, microdata.dtype)\n",
    "#         ) as pool:\n",
    "            \n",
    "#             # Chunk tasks for better load balancing\n",
    "#             tasks = [\n",
    "#                 (i, constraints_dict[\"constraint_targets\"][i], \n",
    "#                 constraints_dict[\"population_totals\"][i])\n",
    "#                 for i in range(len(constraints_dict[\"geography_codes\"]))\n",
    "#             ]\n",
    "            \n",
    "#             # Process with progress bar\n",
    "#             results = []\n",
    "#             for result in tqdm(pool.imap_unordered(worker, tasks, chunksize=4), \n",
    "#                             total=len(tasks)):\n",
    "#                 if result:\n",
    "#                     results.append(result)\n",
    "                    \n",
    "#     finally:\n",
    "#         shm.close()\n",
    "#         shm.unlink()\n",
    "    \n",
    "#     # Analyze results\n",
    "#     avg_steps = np.mean([r['steps'] for r in results])\n",
    "#     print(f\"Completed {len(results)} geographies (avg {avg_steps:.0f} steps each)\")\n",
    "#     print(f\"Final errors: {[r['error'] for r in results[:5]]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf4924-2586-4e62-a8d5-aa9e072c3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b24fc0-6a75-4c12-852a-23c98ef3d305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
