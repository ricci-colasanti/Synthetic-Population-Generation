{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2ec381-d810-45bb-9a75-97235a146ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6161f291-a4cf-4cd0-b0e3-f67ffd3c82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml  # or json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, shared_memory\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477e1c0-1eb4-489d-933f-88fb8031d971",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Synthetic Population Generation via Categorical Constraint Matching**\n",
    "\n",
    "#### **Core Methodology**\n",
    "This implementation transforms raw census constraints and individual microdata into optimized numerical representations for synthetic population generation, using a three-stage process:\n",
    "\n",
    "1. **Constraint Processing**  \n",
    "   - Ingests structured CSV constraints (e.g., `age.csv`, `sex.csv`)  \n",
    "   - Generates:  \n",
    "     - A **label list** (e.g., `['age%16-24', 'age%25-34', 'sex%m', 'sex%f']`)  \n",
    "     - A **target matrix** of census counts per geography zone (`np.array` shape: `[n_zones, n_categories]`)  \n",
    "\n",
    "2. **Microdata Encoding**  \n",
    "   - Converts individual records (`microdata.csv`) into a sparse binary matrix where:  \n",
    "     - Rows = Individuals  \n",
    "     - Columns = Constraint categories  \n",
    "     - Values = `1` (matches category) or `0` (no match/missing)  \n",
    "   - *Example*: An individual with `age=25-34` and `sex=m` encodes as `[0,1,0,0,0,0,1,0]`  \n",
    "\n",
    "3. **Memory-Efficient Design**  \n",
    "   - Uses pure NumPy arrays (no Pandas) for:  \n",
    "     - Zero-copy sharing in multiprocessing  \n",
    "     - O(1) incremental updates during annealing  \n",
    "   - Handles missing data implicitly via zero-padding  \n",
    "\n",
    "#### **Key Innovations**  \n",
    "- **Deterministic Labeling**: Human-readable category prefixes (`age%`, `sex%`) ensure traceability  \n",
    "- **Sparse-by-Design**: Binary encoding minimizes memory overhead  \n",
    "- **Annealing-Ready**: Optimized for rapid constraint violation checks during optimization  \n",
    "\n",
    "#### **Technical Highlights**  \n",
    "```python\n",
    "# Pseudocode of data flow\n",
    "constraint_labels, constraint_targets = build_constraint_arrays(config)  # From age/sex CSVs\n",
    "microdata_encoded = encode_microdata(config, constraint_labels)  # Binary matrix\n",
    "\n",
    "# During annealing:\n",
    "current_error = calculate_error(microdata_encoded, constraint_targets)  # L1/Chi-squared\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of Data Flow**  \n",
    "```mermaid\n",
    "graph LR\n",
    "    A[age.csv] --> C[Constraint Processor]\n",
    "    B[sex.csv] --> C\n",
    "    C --> D[Constraint Labels]\n",
    "    C --> E[Target Matrix]\n",
    "    F[microdata.csv] --> G[Microdata Encoder]\n",
    "    D --> G\n",
    "    G --> H[Binary Encoded Matrix]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Works**  \n",
    "- **Scalability**: Processes 100K+ individuals with minimal memory  \n",
    "- **Flexibility**: New constraints require only YAML updates (no code changes)  \n",
    "- **Reproducibility**: Explicit category mapping avoids hidden assumptions  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3762880-a62b-4129-b3b5-371c23665830",
   "metadata": {},
   "source": [
    "## Example YAML Configuration\n",
    "\n",
    "```yaml\n",
    "# Required microdata source\n",
    "microdata:\n",
    "  file: \"data/microdata.csv\"  # Path to individual records\n",
    "  id_column: \"ID\"            # Optional unique identifier column\n",
    "\n",
    "# List of constraint definitions\n",
    "constraints:\n",
    "  # Age distribution constraints\n",
    "  - file: \"data/age.csv\"             # Census data file\n",
    "    microdata_id: \"Age\"              # Matching column in microdata\n",
    "    constraint_prefix: \"Age%\"        # Label prefix for categories\n",
    "    geography_column: \"GEO_CODE\"    # Zone identifier column\n",
    "\n",
    "  # Sex distribution constraints  \n",
    "  - file: \"data/sex.csv\"\n",
    "    microdata_id: \"Sex\"\n",
    "    constraint_prefix: \"Sex%\"\n",
    "    geography_column: \"GEO_CODE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56aea048-460e-4b81-b922-e0da212706af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load YAML config file and validate structure.\"\"\"\n",
    "    config_path = Path(config_path)\n",
    "    with open(config_path) as f:\n",
    "        if config_path.suffix == '.yaml':\n",
    "            config = yaml.safe_load(f)\n",
    "        else:\n",
    "            import json\n",
    "            config = json.load(f)\n",
    "    \n",
    "    # Validate config structure\n",
    "    assert \"microdata\" in config, \"Config missing 'microdata' section\"\n",
    "    assert \"constraints\" in config and len(config[\"constraints\"]) > 0, \"No constraints defined\"\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8a919-346a-432c-9c0b-dc165ac9a24d",
   "metadata": {},
   "source": [
    "## `load_config(config_path)`\n",
    "\n",
    "**Purpose**:  \n",
    "Loads and validates a configuration file (YAML or JSON) that defines the microdata and constraints structure.\n",
    "\n",
    "**Inputs**:\n",
    "- `config_path` (str or Path): Path to the configuration file (`.yaml` or `.json`)\n",
    "\n",
    "**Returns**:\n",
    "- `dict`: Parsed configuration with keys `'microdata'` and `'constraints'`\n",
    "\n",
    "**Key Features**:\n",
    "- Automatically detects file format (YAML/JSON) from extension\n",
    "- Validates presence of required sections:\n",
    "  - `'microdata'`: File path for microdata CSV\n",
    "  - `'constraints'`: List of constraint definitions\n",
    "- Raises `AssertionError` if structure is invalid\n",
    "\n",
    "**Example Config**:\n",
    "```yaml\n",
    "microdata:\n",
    "  file: \"data/microdata.csv\"\n",
    "constraints:\n",
    "  - file: \"data/age.csv\"\n",
    "    microdata_id: \"age\"\n",
    "    constraint_prefix: \"age%\"\n",
    "    geography_column: \"GEO_CODE\"\n",
    "    set_as_population_total: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3006ffb3-d570-49e8-87c0-4112a957ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'microdata': {'file': 'testdata/microdata.csv', 'id_column': 'ID'}, 'constraints': [{'file': 'testdata/age.csv', 'microdata_id': 'Age', 'constraint_prefix': 'Age%', 'geography_column': 'GEO_CODE', 'set_as_population_total': True}, {'file': 'testdata/sex.csv', 'microdata_id': 'Sex', 'constraint_prefix': 'Sex%', 'geography_column': 'GEO_CODE', 'set_as_population_total': False}]}\n"
     ]
    }
   ],
   "source": [
    "config = load_config('testdata/config.yaml')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a4b2232-1bf9-4c48-abd6-7afe43d746ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_constraint_arrays(config):\n",
    "    \"\"\"\n",
    "    Enhanced version that:\n",
    "    1. Uses set_as_population_total to calculate population sizes\n",
    "    2. Tracks geography codes (GEOIDs) separately\n",
    "    3. Returns results in a structured dict\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"constraint_labels\": List[str],\n",
    "            \"constraint_targets\": np.array,\n",
    "            \"geography_codes\": List[str],\n",
    "            \"population_constraints\": np.array\n",
    "            \"population_totals\":np.arry\n",
    "        }\n",
    "    \"\"\"\n",
    "    constraint_labels = []\n",
    "    constraint_targets = None\n",
    "    geography_codes = []\n",
    "    pop_total_constraint = False\n",
    "    population_constraints = []\n",
    "    population_totals=[]\n",
    "    \n",
    "\n",
    "    for constraint in config[\"constraints\"]:\n",
    "        with open(constraint[\"file\"], mode='r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = next(reader)\n",
    "            data = list(reader)\n",
    "        \n",
    "        poptotal_constraint = constraint[\"set_as_population_total\"]\n",
    "        print(poptotal_constraint)\n",
    "            \n",
    "        geo_col = constraint[\"geography_column\"]\n",
    "        geo_idx = headers.index(geo_col)\n",
    "        \n",
    "        # Store GEOIDs on first pass\n",
    "        if not geography_codes:\n",
    "            geography_codes = [row[geo_idx] for row in data]\n",
    "        \n",
    "        # Handle population totals if specified\n",
    "        if pop_total_constraint: \n",
    "            population_constraints = np.array([float(row[pop_idx]) for row in data])\n",
    "        \n",
    "        # Process categories\n",
    "        categories = [h for i, h in enumerate(headers) if i != geo_idx]\n",
    "        prefix = constraint[\"constraint_prefix\"]\n",
    "        constraint_labels.extend(f\"{prefix}{cat}\" for cat in categories)\n",
    "        \n",
    "        # Extract targets\n",
    "        target_rows = []\n",
    "        for row in data:\n",
    "            target_values = [float(row[i]) for i in range(len(headers)) if i != geo_idx]\n",
    "            total_population = sum(target_values)\n",
    "            if poptotal_constraint:\n",
    "                population_totals.append(total_population)\n",
    "            target_values = [v/total_population for v in target_values]\n",
    "            target_rows.append(target_values)\n",
    "\n",
    "        \n",
    "        targets = np.array(target_rows)\n",
    "        constraint_targets = targets if constraint_targets is None else np.hstack([constraint_targets, targets])\n",
    "    \n",
    "    return {\n",
    "        \"constraint_labels\": constraint_labels,\n",
    "        \"constraint_targets\": constraint_targets.tolist(),\n",
    "        \"geography_codes\": geography_codes,\n",
    "        \"population_totals\":population_totals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99e5001d-8552-4b9b-8a73-0b8749b351b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "constraints_dict = build_constraint_arrays(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "608d2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: constraint_labels Type: list\n",
      "Key: constraint_targets Type: list\n",
      "Key: geography_codes Type: list\n",
      "Key: population_totals Type: list\n"
     ]
    }
   ],
   "source": [
    "for key, value in constraints_dict.items():\n",
    "    print(f\"Key: {key:>10} Type: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca277644-7a5d-4850-b043-2fd19e37eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_dict\n",
    "with open('constraints.json', 'w') as f:\n",
    "    json.dump(constraints_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd8a71-80c9-4f90-a775-5fbb9ac185b0",
   "metadata": {},
   "source": [
    "## `build_constraint_arrays(config)`\n",
    "\n",
    "**Purpose**:  \n",
    "Transforms constraint CSV files (like age/sex distributions) into labeled NumPy arrays for synthetic population generation.\n",
    "\n",
    "### Inputs\n",
    "- `config` (dict): Configuration dictionary containing:\n",
    "  - `constraints`: List of constraint definitions (file paths, prefixes, etc.)\n",
    "\n",
    "### Returns\n",
    "- `constraint_labels` (List[str]): Formatted category labels  \n",
    "  Example: `[\"age%16-24\", \"age%25-34\", \"sex%m\", \"sex%f\"]`\n",
    "- `constraint_targets` (np.array): 2D array of census counts  \n",
    "  Shape: `(n_geographies, n_constraints)`\n",
    "\n",
    "### Key Features\n",
    "-  **File Processing**:\n",
    "  - Reads CSV files without Pandas (vanilla Python `csv` module)\n",
    "  - Handles header rows and geography columns intelligently\n",
    "-  **Smart Labeling**:\n",
    "  - Combines constraint prefixes with category names  \n",
    "    (e.g., `\"age%\" + \"25-34\" → \"age%25-34\"`)\n",
    "-  **Array Construction**:\n",
    "  - Builds a consolidated NumPy array by horizontally stacking constraints\n",
    "  - Automatically converts string values to floats\n",
    "\n",
    "### Example Workflow\n",
    "```python\n",
    "config = {\n",
    "    \"constraints\": [\n",
    "        {\n",
    "            \"file\": \"data/age.csv\",\n",
    "            \"constraint_prefix\": \"age%\",\n",
    "            \"geography_column\": \"GEO_CODE\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "labels, targets = build_constraint_arrays(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d7a37-fcda-4c48-917c-3b17d21e30a0",
   "metadata": {},
   "source": [
    "#### **Key Points**\n",
    "- **`np.hstack`**: Short for \"horizontal stack,\" it concatenates arrays column-wise.  \n",
    "  Example:\n",
    "  ```python\n",
    "  import numpy as np\n",
    "  a = np.array([[1, 2], [3, 4]])\n",
    "  b = np.array([[5], [6]])\n",
    "  np.hstack([a, b])  # Result: [[1, 2, 5], [3, 4, 6]]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3cbf0f0-07c3-4616-b828-1bada3b1a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_microdata(config, constraint_labels):\n",
    "    \"\"\"\n",
    "    Encode microdata into a one-hot-like array where missing values are 0.\n",
    "    Returns:\n",
    "        microdata_encoded: np.array shape (n_individuals, n_constraints)\n",
    "        ids: list of IDs from the microdata\n",
    "    \"\"\"\n",
    "    # Step 1: Load microdata from CSV\n",
    "    with open(config[\"microdata\"][\"file\"], mode='r') as f:\n",
    "        reader = csv.DictReader(f)  # Reads header and rows as dictionaries\n",
    "        microdata = list(reader)    # Convert to list of dicts\n",
    "\n",
    "    n_individuals = len(microdata)\n",
    "    n_constraints = len(constraint_labels)\n",
    "\n",
    "    # Step 2: Create label-to-index mapping\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(constraint_labels)}\n",
    "\n",
    "    # Step 3: Initialize output array (all zeros)\n",
    "    microdata_encoded = np.zeros((n_individuals, n_constraints), dtype=np.int8)\n",
    "\n",
    "    # Step 4: Extract IDs\n",
    "    ids = [row[config[\"microdata\"][\"id_column\"]] for row in microdata]\n",
    "\n",
    "    # Step 5: Encode each constraint\n",
    "    for constraint in config[\"constraints\"]:\n",
    "        col = constraint[\"microdata_id\"]\n",
    "        prefix = constraint[\"constraint_prefix\"]\n",
    "\n",
    "        for row_idx, row in enumerate(microdata):\n",
    "            value = row.get(col)  # Get value for the current constraint column\n",
    "\n",
    "            # Skip missing values (leave as 0)\n",
    "            if value is not None and value.strip() != '':  # Check for non-empty strings\n",
    "                label = f\"{prefix}{value}\"\n",
    "                if label in label_to_idx:  # Ensure label exists in constraints\n",
    "                    microdata_encoded[row_idx, label_to_idx[label]] = 1\n",
    "\n",
    "    return {\n",
    "        \"microdata_encoded\":microdata_encoded, \n",
    "        \"ids\":ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e3cf36-63cf-4eaf-9836-947d2eb80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "microdata_dict = encode_microdata(config,constraints_dict[\"constraint_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc89c4-42e4-48ef-93f3-41a7fabeb49c",
   "metadata": {},
   "source": [
    "##  `encode_microdata(config, constraint_labels)`\n",
    "\n",
    "**Purpose**:  \n",
    "Converts individual microdata records into a binary matrix matching census constraints, with automatic handling of missing values.\n",
    "\n",
    "### Inputs\n",
    "- `config` (dict): Configuration dictionary with microdata file path\n",
    "- `constraint_labels` (List[str]): Pre-generated labels from `build_constraint_arrays()`\n",
    "\n",
    "### Returns\n",
    "- `microdata_encoded` (np.array): Binary matrix where:\n",
    "  - Rows = Individuals\n",
    "  - Columns = Constraint categories\n",
    "  - Values = `1` (present) or `0` (missing/not applicable)\n",
    "\n",
    "### Key Features\n",
    "-   **Smart Encoding**:\n",
    "  - Converts categorical values (e.g., `\"m\"`, `\"25-34\"`) to binary flags\n",
    "  - Preserves relationships between original data and constraint categories\n",
    "-   **Missing Data Handling**:\n",
    "  - Empty/missing values remain `0` by default\n",
    "  - Silent skipping of undefined categories\n",
    "-   **Efficient Construction**:\n",
    "  - Pre-allocates NumPy array for performance\n",
    "  - Uses memory-efficient `int8` dtype\n",
    "\n",
    "### Example Transformation\n",
    "**Input Microdata**:\n",
    "```csv\n",
    "ID,sex,age\n",
    "1,m,25-34\n",
    "2,f,55-64\n",
    "3,,45-54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89d307f-7abd-4225-8861-fdcbab056853",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('testdata/microdata_encoded.csv', microdata_dict[\"microdata_encoded\"], delimiter=',')\n",
    "np.savetxt('testdata/constraint_targets.csv', constraints_dict[\"constraint_targets\"], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "888650a4-2a0f-44f7-97c1-e3b4f06c8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelAnnealingGenerator:\n",
    "    def __init__(self, target_distribution, population_size, micro_data, \n",
    "                 initial_temp=100.0, cooling_rate=0.99):\n",
    "        \"\"\"\n",
    "        Parallel-safe implementation with proper annealing controls\n",
    "        \n",
    "        Args:\n",
    "            target_distribution: 1D np.array of target category counts\n",
    "            population_size: Number of individuals to select\n",
    "            micro_data: Shared read-only microdata array\n",
    "            initial_temp: Starting temperature (default 100.0)\n",
    "            cooling_rate: Geometric cooling rate (default 0.99)\n",
    "        \"\"\"\n",
    "        self.target = target_distribution\n",
    "        self.pop_size = int(population_size)\n",
    "        self.full_data = micro_data  # Shared read-only reference\n",
    "        \n",
    "        # Annealing controls\n",
    "        self.temp = float(initial_temp)\n",
    "        self.base_temp = float(initial_temp)\n",
    "        self.cooling = float(cooling_rate)\n",
    "        \n",
    "        # Initialize population (protected copy)\n",
    "        self.indices = np.random.choice(len(micro_data), size=self.pop_size, replace=True)\n",
    "        self.local_pop = micro_data[self.indices].copy()  # Isolated working copy\n",
    "        self.counts = self.local_pop.sum(axis=0)\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_indices = self.indices.copy()\n",
    "        self.best_counts = self.counts.copy()\n",
    "        self.best_error = self._calculate_error(self.counts)\n",
    "\n",
    "    def _calculate_error(self, counts):\n",
    "        \"\"\"Compute L1 distance between current and target distributions\"\"\"\n",
    "        return np.sum(np.abs(counts - self.target))\n",
    "\n",
    "    def optimization_step(self):\n",
    "        # 1. Select replacement candidate\n",
    "        replace_pos = np.random.randint(self.pop_size)\n",
    "        old_features = self.local_pop[replace_pos]\n",
    "        \n",
    "        # 2. Get new candidate safely from shared array\n",
    "        new_idx = np.random.randint(len(self.full_data))\n",
    "        new_features = self.full_data[new_idx]  # Atomic read\n",
    "        \n",
    "        # 3. Calculate delta\n",
    "        delta = new_features - old_features\n",
    "        new_counts = self.counts + delta\n",
    "        new_error = self._calculate_error(new_counts)\n",
    "        error_delta = new_error - self.best_error\n",
    "        \n",
    "        # 4. Metropolis acceptance\n",
    "        if error_delta < 0 or np.random.rand() < np.exp(-error_delta / self.temp):\n",
    "            self.counts = new_counts\n",
    "            self.local_pop[replace_pos] = new_features\n",
    "            self.indices[replace_pos] = new_idx\n",
    "            \n",
    "            if new_error < self.best_error:\n",
    "                self.best_error = new_error\n",
    "                self.best_indices = self.indices.copy()\n",
    "                self.best_counts = self.counts.copy()\n",
    "        \n",
    "        # 5. Apply cooling\n",
    "        self.temp *= self.cooling\n",
    "        return self.best_error\n",
    "\n",
    "    def reset_annealing(self, new_temp=None):\n",
    "        \"\"\"Reset temperature for new runs\"\"\"\n",
    "        self.temp = float(new_temp) if new_temp else self.base_temp\n",
    "        # Optional: Keep best solution or re-randomize\n",
    "        # self.indices = self.best_indices.copy()\n",
    "\n",
    "    def get_results(self):\n",
    "        return {\n",
    "            'selected_indices': self.best_indices,\n",
    "            'population_counts': self.best_counts,\n",
    "            'target_counts': self.target,\n",
    "            'error': self.best_error,\n",
    "            'final_temperature': self.temp\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae439d-858b-44ce-a5cb-f2823922df21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f231c-f73d-44a7-a8c3-acd0cf7f32ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage pattern matching your example:\n",
    "print(\"start\")\n",
    "for i in range(len(constraints_dict[\"geography_codes\"])):\n",
    "    # Initialize generator for this geography\n",
    "    generator = ParallelAnnealingGenerator(\n",
    "        constraints_dict[\"constraint_targets\"][i],\n",
    "        constraints_dict[\"population_constraints\"][i],  # Note: Fixed typo from your original\n",
    "        microdata_dict[\"microdata_encoded\"],\n",
    "        100,  # Initial temp\n",
    "        0.99   # Cooling rate\n",
    "    )\n",
    "    \n",
    "    # Initial error threshold (10% of first step)\n",
    "    threshold = generator.optimization_step() / 10\n",
    "    \n",
    "    # Run until convergence or max iterations\n",
    "    for c in range(20000):\n",
    "        current_error = generator.optimization_step()\n",
    "        if current_error <= threshold :#or generator.temp <0.000001:\n",
    "            break\n",
    "    \n",
    "    results = generator.get_results()\n",
    "    # print(f\"Geography {constraints_dict['geography_codes'][i]}:\")\n",
    "    # print(f\"- Final error: {results['error']:.1f}\")\n",
    "    # print(f\"pop: {len(results['selected_indices'])} - Sample indices: {results['selected_indices'][:5]} \")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594de7c9-1f18-47e4-bc2d-9541dda34645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  # For progress tracking\n",
    "\n",
    "# def init_worker(shared_mem_name, shape, dtype):\n",
    "#     \"\"\"Initialize worker with shared memory\"\"\"\n",
    "#     global shared_microdata\n",
    "#     shm = shared_memory.SharedMemory(name=shared_mem_name)\n",
    "#     shared_microdata = np.ndarray(shape, dtype=dtype, buffer=shm.buf)\n",
    "\n",
    "# def worker(args):\n",
    "#     geo_idx, target, pop_size = args\n",
    "#     try:\n",
    "#         # Initialize with more aggressive cooling\n",
    "#         generator = ParallelAnnealingGenerator(\n",
    "#             target_distribution=target,\n",
    "#             population_size=pop_size,\n",
    "#             micro_data=shared_microdata,\n",
    "#             initial_temp=50.0,  # Lower initial temp\n",
    "#             cooling_rate=0.95    # Faster cooling\n",
    "#         )\n",
    "        \n",
    "#         # Dynamic threshold based on population size\n",
    "#         threshold = max(5, pop_size * 0.01)  # At least 5, max 1% of pop size\n",
    "        \n",
    "#         # Optimized stopping conditions\n",
    "#         for step in range(20000):\n",
    "#             print(step,geo_idx)\n",
    "#             error = generator.optimization_step()\n",
    "#             if error <= threshold or generator.temp < 0.1:\n",
    "#                 break\n",
    "                \n",
    "#         return {\n",
    "#             'geo_idx': geo_idx,\n",
    "#             'error': error,\n",
    "#             'steps': step,\n",
    "#             'final_temp': generator.temp\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in geography {geo_idx}: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Prepare shared memory (10-20% faster than automatic pickling)\n",
    "#     microdata = microdata_dict[\"microdata_encoded\"]\n",
    "#     shm = shared_memory.SharedMemory(create=True, size=microdata.nbytes)\n",
    "#     shared_array = np.ndarray(microdata.shape, dtype=microdata.dtype, buffer=shm.buf)\n",
    "#     shared_array[:] = microdata[:]\n",
    "\n",
    "#     try:\n",
    "#         # Optimal worker count (leave 1 core free)\n",
    "#         n_workers = max(1, os.cpu_count() - 1)\n",
    "#         print(f\"Using {n_workers} workers with shared memory\")\n",
    "        \n",
    "#         with Pool(\n",
    "#             processes=n_workers,\n",
    "#             initializer=init_worker,\n",
    "#             initargs=(shm.name, microdata.shape, microdata.dtype)\n",
    "#         ) as pool:\n",
    "            \n",
    "#             # Chunk tasks for better load balancing\n",
    "#             tasks = [\n",
    "#                 (i, constraints_dict[\"constraint_targets\"][i], \n",
    "#                 constraints_dict[\"population_constraints\"][i])\n",
    "#                 for i in range(len(constraints_dict[\"geography_codes\"]))\n",
    "#             ]\n",
    "            \n",
    "#             # Process with progress bar\n",
    "#             results = []\n",
    "#             for result in tqdm(pool.imap_unordered(worker, tasks, chunksize=4), \n",
    "#                             total=len(tasks)):\n",
    "#                 if result:\n",
    "#                     results.append(result)\n",
    "                    \n",
    "#     finally:\n",
    "#         shm.close()\n",
    "#         shm.unlink()\n",
    "    \n",
    "#     # Analyze results\n",
    "#     avg_steps = np.mean([r['steps'] for r in results])\n",
    "#     print(f\"Completed {len(results)} geographies (avg {avg_steps:.0f} steps each)\")\n",
    "#     print(f\"Final errors: {[r['error'] for r in results[:5]]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf4924-2586-4e62-a8d5-aa9e072c3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b24fc0-6a75-4c12-852a-23c98ef3d305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
